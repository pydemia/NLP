{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Scrapper with Multi-core : by Media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import itertools as it\n",
    "import pickle\n",
    "import multiprocessing as mpr\n",
    "import datetime as dt\n",
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from enum import Enum, unique\n",
    "\n",
    "import feedparser\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "import nltk\n",
    "import konlpy as knlp\n",
    "from konlpy.corpus import kolaw, kobill\n",
    "from konlpy.tag import Hannanum, Kkma, Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Create links of News-List page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def news_link_extractor(source, date):\n",
    "\n",
    "    chosun_url_dict = {\n",
    "    'chosun_economics':\n",
    "    'http://news.chosun.com/svc/list_in/list.html?catid=1&source=1&indate={date}&pn={pg_num}',\n",
    "\n",
    "    'chosun_politics':\n",
    "    'http://news.chosun.com/svc/list_in/list.html?catid=2&source=1&indate={date}&pn={pg_num}',\n",
    "\n",
    "    'chosun_national':\n",
    "    'http://news.chosun.com/svc/list_in/list.html?catid=3&source=1&indate={date}&pn={pg_num}',\n",
    "\n",
    "    'chosun_international':\n",
    "    'http://news.chosun.com/svc/list_in/list.html?catid=4&source=1&indate={date}&pn={pg_num}',\n",
    "\n",
    "    'chosun_cultures':\n",
    "    'http://news.chosun.com/svc/list_in/list.html?catid=5&source=1&indate={date}&pn={pg_num}',\n",
    "\n",
    "    'chosun_opinions':\n",
    "    'http://news.chosun.com/svc/list_in/list.html?catid=6&source=1&indate={date}&pn={pg_num}',\n",
    "\n",
    "    'chosun_accidents':\n",
    "    'http://news.chosun.com/svc/list_in/list.html?catid=7&source=1&indate={date}&pn={pg_num}',\n",
    "\n",
    "    'chosun_sports':\n",
    "    'http://news.chosun.com/svc/list_in/list.html?catid=G1&source=1&indate={date}&pn={pg_num}',\n",
    "\n",
    "    'chosun_entertainments':\n",
    "    'http://news.chosun.com/svc/list_in/list.html?catid=G1&source=1&indate={date}&pn={pg_num}',\n",
    "                     }\n",
    "    \n",
    "    press_dict = {'chosun': chosun_url_dict,\n",
    "             'hani': None}\n",
    "    \n",
    "    src_dict = press_dict[source]\n",
    "    url_dict = {'%s_%s' % (key, pg_num):\n",
    "                src_dict[key].format(date=date, pg_num=pg_num)\\\n",
    "                for key, pg_num in it.product(src_dict, [1, 2])}\n",
    "\n",
    "    # Get the Full article of the Feeds\n",
    "#    pool = mpr.pool.Pool(processes=worker)\n",
    "#    response = pool.map(link_parser, feed_list)\n",
    "#    news_list = sum(response, [])\n",
    "    return url_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chosun_accidents_1': 'http://news.chosun.com/svc/list_in/list.html?catid=7&source=1&indate=20171128&pn=1',\n",
       " 'chosun_accidents_2': 'http://news.chosun.com/svc/list_in/list.html?catid=7&source=1&indate=20171128&pn=2',\n",
       " 'chosun_cultures_1': 'http://news.chosun.com/svc/list_in/list.html?catid=5&source=1&indate=20171128&pn=1',\n",
       " 'chosun_cultures_2': 'http://news.chosun.com/svc/list_in/list.html?catid=5&source=1&indate=20171128&pn=2',\n",
       " 'chosun_economics_1': 'http://news.chosun.com/svc/list_in/list.html?catid=1&source=1&indate=20171128&pn=1',\n",
       " 'chosun_economics_2': 'http://news.chosun.com/svc/list_in/list.html?catid=1&source=1&indate=20171128&pn=2',\n",
       " 'chosun_entertainments_1': 'http://news.chosun.com/svc/list_in/list.html?catid=G1&source=1&indate=20171128&pn=1',\n",
       " 'chosun_entertainments_2': 'http://news.chosun.com/svc/list_in/list.html?catid=G1&source=1&indate=20171128&pn=2',\n",
       " 'chosun_international_1': 'http://news.chosun.com/svc/list_in/list.html?catid=4&source=1&indate=20171128&pn=1',\n",
       " 'chosun_international_2': 'http://news.chosun.com/svc/list_in/list.html?catid=4&source=1&indate=20171128&pn=2',\n",
       " 'chosun_national_1': 'http://news.chosun.com/svc/list_in/list.html?catid=3&source=1&indate=20171128&pn=1',\n",
       " 'chosun_national_2': 'http://news.chosun.com/svc/list_in/list.html?catid=3&source=1&indate=20171128&pn=2',\n",
       " 'chosun_opinions_1': 'http://news.chosun.com/svc/list_in/list.html?catid=6&source=1&indate=20171128&pn=1',\n",
       " 'chosun_opinions_2': 'http://news.chosun.com/svc/list_in/list.html?catid=6&source=1&indate=20171128&pn=2',\n",
       " 'chosun_politics_1': 'http://news.chosun.com/svc/list_in/list.html?catid=2&source=1&indate=20171128&pn=1',\n",
       " 'chosun_politics_2': 'http://news.chosun.com/svc/list_in/list.html?catid=2&source=1&indate=20171128&pn=2',\n",
       " 'chosun_sports_1': 'http://news.chosun.com/svc/list_in/list.html?catid=G1&source=1&indate=20171128&pn=1',\n",
       " 'chosun_sports_2': 'http://news.chosun.com/svc/list_in/list.html?catid=G1&source=1&indate=20171128&pn=2'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = news_link_extractor('chosun', '20171128')\n",
    "aa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Crawl and Make-up the feeds of the news pages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_parser(url):\n",
    "    page = urllib.request.urlopen(url)\n",
    "    soup = bs(page, \"html5lib\")\n",
    "    html_list = soup.find_all('dl', attrs={'class': 'list_item'})\n",
    "    link_list = [(link.a.text, link.a['href']) for link in html_list]\n",
    "    return link_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_news_datetime(url):\n",
    "    # Chosun\n",
    "    page = urllib.request.urlopen(url)\n",
    "    soup = bs(page, \"html5lib\")\n",
    "    pattern = re.compile(r'var TempDate=.+ : (\\d+.\\d+.\\d+ \\d+:\\d+)\"$', re.MULTILINE | re.DOTALL)\n",
    "    script = soup.find(\"script\", text=pattern)\n",
    "    dt_tm_str = pattern.findall(script.text)[0]\n",
    "    dt_tm = dt.datetime.strptime(dt_tm_str, '%Y.%m.%d %H:%M').strftime('%Y-%m-%d %H:%M:%S')\n",
    "    return dt_tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_news_title(url):\n",
    "    # Chosun\n",
    "    page = urllib.request.urlopen(url)\n",
    "    soup = bs(page, \"html5lib\")\n",
    "    pattern = re.compile(r'var ArtTitle =\\s*(.+)\";$')\n",
    "    script = soup.find(\"script\", text=pattern)\n",
    "    title_str = pattern.findall(script.text)[0]\n",
    "    return title_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def link_crawler_single(url_dict):\n",
    "\n",
    "    full_link_gen = ((key, link_parser(url_dict[key])) for key in url_dict)\n",
    "    \n",
    "    link_list = [{'press': key.split('_')[0],\n",
    "                  'topic': key.split('_')[1],\n",
    "                  'link': link,\n",
    "                  'title': title,\n",
    "                  'datetime': get_news_datetime(link)}\\\n",
    "                 for key, link_list in full_link_gen for title, link in link_list]\n",
    "\n",
    "    return link_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def link_crawler(key, url):\n",
    "\n",
    "    full_link_tuple = ((key, link_parser(url)), )\n",
    "    link_list = [{'press': key.split('_')[0],\n",
    "                  'topic': key.split('_')[1],\n",
    "                  'link': link,\n",
    "                  'title': title,\n",
    "                  'datetime': get_news_datetime(link)}\\\n",
    "                 for key, link_list in full_link_tuple for title, link in link_list]\n",
    "\n",
    "    return link_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'datetime': '2017-11-28 22:42:00',\n",
       "  'link': 'http://news.chosun.com/site/data/html_dir/2017/11/29/2017112900506.html',\n",
       "  'press': 'chosun',\n",
       "  'title': 'LH 단지내 상가 입찰 105억 뭉칫돈 몰렸네',\n",
       "  'topic': 'economics'},\n",
       " {'datetime': '2017-11-28 22:22:00',\n",
       "  'link': 'http://news.chosun.com/site/data/html_dir/2017/11/29/2017112900500.html',\n",
       "  'press': 'chosun',\n",
       "  'title': '미친 분양가… 3.3㎡당 1억원 찍나',\n",
       "  'topic': 'economics'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb = link_crawler_single(aa)\n",
    "bb[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'datetime': '2017-11-28 22:42:00',\n",
       "  'link': 'http://news.chosun.com/site/data/html_dir/2017/11/29/2017112900506.html',\n",
       "  'press': 'chosun',\n",
       "  'title': 'LH 단지내 상가 입찰 105억 뭉칫돈 몰렸네',\n",
       "  'topic': 'economics'},\n",
       " {'datetime': '2017-11-28 22:22:00',\n",
       "  'link': 'http://news.chosun.com/site/data/html_dir/2017/11/29/2017112900500.html',\n",
       "  'press': 'chosun',\n",
       "  'title': '미친 분양가… 3.3㎡당 1억원 찍나',\n",
       "  'topic': 'economics'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa_items = [(key, item) for key, item in aa.items()]\n",
    "bb = link_crawler(aa_items[0][0], aa_items[0][1])\n",
    "bb[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Scrap the News Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def article_html_reader(url, source):\n",
    "    \n",
    "    src_dict = {'chosun': None,\n",
    "                'hani': None,\n",
    "                'mk': None,\n",
    "                'hk': None}\n",
    "\n",
    "    try:\n",
    "        page = urllib.request.urlopen(url)\n",
    "        soup = bs(page, \"html5lib\")\n",
    "\n",
    "        if source == 'chosun':\n",
    "            raw_html = raw_html = soup.find_all('div', attrs={'par', 'article'})\n",
    "\n",
    "        elif source == 'hani':\n",
    "            raw_html = soup.find_all('div', attrs='text')\n",
    "            \n",
    "        elif source == 'mk':\n",
    "            raw_html = soup.find_all('div', attrs='art_txt')\n",
    "\n",
    "        elif source == 'hk':\n",
    "            #raw_html = str(soup.find_all('div', attrs={'news_article', 'wrap_article'}))[1:-1]\n",
    "            raw_html = soup.find_all('div', attrs={'wrap_article', 'news_article'})\n",
    "\n",
    "    except urllib.error.HTTPError as e:\n",
    "        print(url, str(e))\n",
    "        raw_html = None\n",
    "    \n",
    "    # For to check the progress\n",
    "    #print('%s (%s)' % (source, url))\n",
    "    \n",
    "    return raw_html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def article_string_extractor(bs_soup):\n",
    "    \n",
    "    if bs_soup.find('script'):\n",
    "        bs_soup.script.decompose()\n",
    "\n",
    "    if bs_soup.find('div', attrs='center_img'):\n",
    "        bs_soup.find('div', attrs='center_img').decompose()\n",
    "\n",
    "    if bs_soup.find('div'):    \n",
    "        bs_soup.find('div').decompose()\n",
    "        \n",
    "    item_str = str(bs_soup)\n",
    "\n",
    "    # Extraction\n",
    "    pattern = re.compile(r'<[^<>]*>')\n",
    "    extracted = ''.join(re.split(pattern, item_str))\n",
    "\n",
    "    # Substitution\n",
    "    pattern = re.compile(r\"\\n|\\t|\\s+$\")\n",
    "    substituted = re.sub(pattern, '', extracted)\n",
    "\n",
    "    sliced = substituted\n",
    "\n",
    "    return sliced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def news_scrapper(url, source):\n",
    "\n",
    "    html_list = article_html_reader(url, source)\n",
    "    news_str = ''.join([article_string_extractor(item) for item in html_list])\n",
    "    \n",
    "    return news_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrap_from_link(feed_list):\n",
    "    res = [dict(feed, **{'body': news_scrapper(feed['link'], feed['press'])})\\\n",
    "           for feed in feed_list]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'body': 'LH(한국토지주택공사)가 최근 공급한 단지 내 상가 입찰에 105억원의 뭉칫돈이 몰렸다.28일 상가정보연구소에 따르면, 지난 20~24일 진행된 LH 단지 내 상가의 입찰에서 37실 중 36실이 낙찰됐다. 낙찰가 총액은 105억5464만원, 평균 낙찰가율은 135.3%였다.경남 양산시 양산물금2지구 H1블록 103호는 예정가 1억3500만원보다 1억1500여만원 비싼 2억5065만5500원에 낙찰돼 이번 공급 상가 중 최고 낙찰가율(186%)을 기록했다. 예정가 6억3000만원이던 같은 상가 204호는 유찰됐다.LH 단지 내 상가는 일반 상가보다 가격이 저렴해 상대적으로 소액 투자가 가능하고, 고정 수요가 탄탄한 것이 장점으로 꼽힌다. 이 때문에 상반기 공급된 LH 단지 내 상가는 평균 낙찰가율이 200%에 육박하기도 했다. 1억원짜리를 2억원에 사들였다는 뜻이다. 그러나 최근 들어 시중금리가 오르면서 상가 등 수익형 부동산 투자 열기가 한풀 꺾일 것이라는 전망이 나온다.',\n",
       "  'datetime': '2017-11-28 22:42:00',\n",
       "  'link': 'http://news.chosun.com/site/data/html_dir/2017/11/29/2017112900506.html',\n",
       "  'press': 'chosun',\n",
       "  'title': 'LH 단지내 상가 입찰 105억 뭉칫돈 몰렸네',\n",
       "  'topic': 'economics'},\n",
       " {'body': '서울 한남동 외인아파트 부지에 들어서는 고급 아파트 \\'나인원 한남\\'의 분양가가 3.3㎡당 최고 1억원에 달한다는 사실이 알려지자 고분양가 논란이 다시 불거지고 있다. 일부에서는 \"\\'높은 분양가→주변 시세 상승→더 높은 분양가\\'로 이어지는 악순환을 끊지 않으면 집값 안정은 불가능하다\"는 목소리도 나온다. 정부도 최근 들어 HUG(주택도시보증공사)를 통한 사실상 분양가 통제에 나서고 있다. 하지만 \"분양가 통제가 \\'로또 아파트\\'를 양산할 수 있으며, 다양성 차원에서도 고급 아파트를 별개 시장으로 인정해야 한다\"는 반론도 만만치 않다.◇3.3㎡당 1억원 시대 눈앞올 상반기까지만 해도 역대 최고 분양가 기록은 서울 성수동 \\'갤러리아 포레\\'가 가지고 있었다. 이 단지는 2008년 3.3㎡당 평균 4535만원에 분양한 이래 10년 가까이 1위 자리를 지켜왔다. 하지만 지난 7월 대림산업이 같은 지역에서 분양한 \\'아크로 서울포레스트\\'(3.3㎡당 4750만원)가 1위를 차지했다.다음 달 분양 예정인 \\'나인원 한남\\'이 계획한 가격대로 주택도시보증공사의 분양 보증을 받아내면 1위가 또 바뀐다. 시행사인 대신F&amp;I가 희망하는 3.3㎡당 평균 분양가는 6000만원 수준이다. 특히 이 단지 각 동(棟) 꼭대기 층 \\'수퍼펜트하우스\\' 분양가는 최고 1억원 안팎인 것으로 알려졌다. 업계에서는 \"서울 각지에서 \\'나인원 한남\\'처럼 \\'고급 주택\\'을 표방하는 아파트 가격이 함께 올라갈 것\"이라고 말한다. 실제로 작년 8월 1조원에 팔린 이태원동 유엔사 부지 자리에 들어설 아파트는 \\'나인원 한남\\'의 기록을 무난히 깨뜨릴 것이란 전망이 벌써부터 나오고 있다.분양가 상승은 일부 부촌(富村)만의 현상이 아니다. 최근 3~4년 사이 아파트 분양가는 전국적으로 오르고 있다. 부동산 정보 업체 \\'부동산114\\'에 따르면 올해 전국 아파트 3.3㎡당 분양가는 전년(1052만원) 대비 9.98% 오른 1157만원을 기록 중이다. 통계 작성이 시작된 2000년 이후 가장 높은 수치다.서울 아파트 분양가는 전년 대비 5.7%가 올라 2253만원이다. 강남권은 4000만원대 분양이 잇따르고 있다. 경기도 역시 작년 대비 7.75% 오른 1223만원을 기록 중이다.◇\"과열 주범\" vs. \"다양성 인정해야\"\\'나인원 한남\\'이 초고가를 표방하고 있지만 실제로 서울에서 분양가를 경쟁적으로 끌어올리는 세력은 아파트 재건축조합이다. 일반 분양분 아파트를 비싸게 팔수록 조합원들이 내야 할 추가 분담금이 적어지기 때문이다.지난 9월 3.3㎡당 4200만원에 분양된 신반포 센트럴 자이의 경우 조합원들은 3.3㎡당 4600만~4700만원을 주장했던 것으로 알려졌다. 건설사도 이에 편승, \\'고분양가\\'를 약속해 조합의 환심을 산다. 최근 논란 끝에 반포주공1단지를 수주한 현대건설은 조합에 \\'최소 분양가 5100만원\\'을 제시한 것이 효과를 봤다는 평가다.건설 업체가 가져가는 \\'건축비\\'를 고분양가 원흉으로 지적하는 의견도 있다. 분양가는 대지비와 건축비로 구성되는데, 경제정의실천시민연합(경실련)이 최근 아파트 분양 공고문을 분석한 결과 서울 강남권에서 2013년 3.3㎡당 634만원(래미안 대치팰리스)이었던 건축비가 2017년엔 1541만원(신반포 센트럴자이)으로 배(倍) 이상 뛰었다는 것이다. 하지만 건설사 측은 \"터무니없는 소리\"라는 입장이다. GS건설 측은 \"공고문 건축비는 감정평가사가 일방적으로 정한 값일 뿐이며, 실제 우리가 센트럴자이를 지으며 받는 공사비는 3.3㎡당 500만원선\"이라고 밝혔다.이런 가운데 정부는 지난 7일 분양가 상한제 적용 조건을 대폭 완화했다. 서울 강남권 등에서는 마음만 먹으면 분양가 상한제 시행이 가능한 상태다. 국토부 관계자는 \"고분양가를 집값 과열의 한 축으로 보고 시장을 주시하는 상황\"이라고 말했다.하지만 정부의 분양가 통제에 반대하는 목소리도 많다. 김승배 피데스개발 대표는 \"사실 지금도 강남 아파트 중 일부 평형은 평당 1억원에 거래된다\"며 \"유독 분양가에만 민감할 이유가 없으며, 분양가도 일반 아파트 시세에 맞춰지는 게 당연한 것\"이라고 했다. 심교언 건국대 교수는 \"분양가를 낮춰놔도 결국엔 주변 시세에 맞춰 가격이 따라 올라가기 때문에 분양받은 사람의 시세 차익만 늘려주는 꼴\"이라며 \"고가(高價) 주택에까지 일괄적인 잣대를 들이대는 것은 똑같은 아파트만 찍어내자는 것으로 다양성 차원에서도 바람직하지 않다\"고 말했다.',\n",
       "  'datetime': '2017-11-28 22:22:00',\n",
       "  'link': 'http://news.chosun.com/site/data/html_dir/2017/11/29/2017112900500.html',\n",
       "  'press': 'chosun',\n",
       "  'title': '미친 분양가… 3.3㎡당 1억원 찍나',\n",
       "  'topic': 'economics'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc = scrap_from_link(bb)\n",
    "cc[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Dump the News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_list_scrapper(media, date, worker=1):\n",
    "\n",
    "    url_dict = news_link_extractor('chosun', date)\n",
    "\n",
    "    # Get the RSS Feeds\n",
    "    # Get the Full article of the Feeds\n",
    "    pool = mpr.pool.Pool(processes=worker)\n",
    "    response = pool.starmap(link_crawler, url_dict.items())\n",
    "    feed_list = response\n",
    "\n",
    "    # Print Lengths of each Feeds\n",
    "    feed_count_dict = {press: len(feed) for press, feed in zip(url_dict, feed_list)}\n",
    "    feed_count = sum(feed_count_dict.values())\n",
    "    print(\"Scrapping '%s, %s' : %s\" % (media, date, feed_count))\n",
    "    #pprint(feed_count_dict)\n",
    "\n",
    "    \n",
    "    # Get the Full article of the Feeds\n",
    "    #print(\"Scrapping '%s'...\" % date)\n",
    "    pool = mpr.pool.Pool(processes=worker)\n",
    "    response = pool.map(scrap_from_link, feed_list)\n",
    "    news_list = sum(response, [])\n",
    "    #print(\"\\rScrapping '%s'...Done\" % date)\n",
    "\n",
    "    return news_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_list_dumper(media, dirname, start_dt, end_dt, worker=1):\n",
    "\n",
    "    date_list = [date.strftime('%Y%m%d')\\\n",
    "                 for date in pd.date_range(start=start_dt, end=end_dt)]\n",
    "\n",
    "    news_list = sum([news_list_scrapper(media, date, worker=worker)\\\n",
    "                          for date in date_list], [])\n",
    "\n",
    "    total_cnt = len(news_list)\n",
    "    print(\"Media    : %s\" % media)\n",
    "    print(\"Date     : %s ~ %s\" % (start_dt, end_dt))\n",
    "    print(\"Total    : %s\" % total_cnt)\n",
    "    os.makedirs(dirname, exist_ok=True)\n",
    "    date_str = dt.datetime.now().strftime('%Y%m%d')\n",
    "    filename = '/'.join([dirname, 'newsdata'])\n",
    "    print(\"Dumping '%s' article(s) to '%s.*' ...\" % (total_cnt, filename))\n",
    "    \n",
    "    # Dump the result\n",
    "    with open('%s.json' % filename, \"w\") as json_file:\n",
    "        json_file.write(json.dumps(news_list))\n",
    "\n",
    "    data = pd.DataFrame.from_dict(news_list)\n",
    "    full_txt = '\\n'.join(data['body'])\n",
    "    with open('%s.txt' % filename, 'w') as text_file:\n",
    "        text_file.write(full_txt)\n",
    "    \n",
    "    full_body_list = data['body'].tolist()\n",
    "    with open('%s.dump' % filename, 'wb') as byte_file:\n",
    "        pickle.dump(full_body_list, byte_file)\n",
    "\n",
    "    print(\"Dumping '%s' article(s) to '%s.*' ...Done\" % (total_cnt, filename))\n",
    "\n",
    "    return news_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dump_name = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapping 'chosun, 20171101' : 113\n",
      "Scrapping 'chosun, 20171102' : 108\n",
      "Scrapping 'chosun, 20171103' : 114\n",
      "Scrapping 'chosun, 20171104' : 91\n",
      "Scrapping 'chosun, 20171105' : 4\n",
      "Scrapping 'chosun, 20171106' : 85\n",
      "Scrapping 'chosun, 20171107' : 88\n",
      "Scrapping 'chosun, 20171108' : 108\n",
      "Scrapping 'chosun, 20171109' : 105\n",
      "Scrapping 'chosun, 20171110' : 108\n",
      "Scrapping 'chosun, 20171111' : 95\n",
      "Scrapping 'chosun, 20171112' : 2\n",
      "Scrapping 'chosun, 20171113' : 102\n",
      "Scrapping 'chosun, 20171114' : 100\n",
      "Scrapping 'chosun, 20171115' : 102\n",
      "Scrapping 'chosun, 20171116' : 87\n",
      "Scrapping 'chosun, 20171117' : 97\n",
      "Scrapping 'chosun, 20171118' : 87\n",
      "Scrapping 'chosun, 20171119' : 5\n",
      "Scrapping 'chosun, 20171120' : 104\n",
      "Scrapping 'chosun, 20171121' : 123\n",
      "Scrapping 'chosun, 20171122' : 95\n",
      "Scrapping 'chosun, 20171123' : 96\n",
      "Scrapping 'chosun, 20171124' : 109\n",
      "Scrapping 'chosun, 20171125' : 101\n",
      "Scrapping 'chosun, 20171126' : 3\n",
      "Scrapping 'chosun, 20171127' : 103\n",
      "Scrapping 'chosun, 20171128' : 101\n",
      "Scrapping 'chosun, 20171129' : 101\n",
      "Scrapping 'chosun, 20171130' : 108\n",
      "Media    : chosun\n",
      "Date     : 20171101 ~ 20171130\n",
      "Total    : 2645\n",
      "Dumping '2645' article(s) to 'test/newsdata.*' ...\n",
      "Dumping '2645' article(s) to 'test/newsdata.*' ...Done\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    test = news_list_dumper('chosun', dump_name, '20171101', '20171130',  worker=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dump_name = 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame from `json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>datetime</th>\n",
       "      <th>link</th>\n",
       "      <th>press</th>\n",
       "      <th>title</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>우리나라의 에너지 자원 해외 수입 의존도는 96%에 달한다. 특히 액화천연가스(LN...</td>\n",
       "      <td>2017-11-01 17:30:00</td>\n",
       "      <td>http://news.chosun.com/site/data/html_dir/2017...</td>\n",
       "      <td>chosun</td>\n",
       "      <td>전 세계 13개국서 탐사·개발… 모잠비크선 \"금세기 최대 규모\"</td>\n",
       "      <td>economics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'공정한 병역(兵役) 실현'을 최우선 과제로 삼은 병무청이 올해 초부터 병역 판정 ...</td>\n",
       "      <td>2017-11-01 17:18:00</td>\n",
       "      <td>http://news.chosun.com/site/data/html_dir/2017...</td>\n",
       "      <td>chosun</td>\n",
       "      <td>총 25개 항목의 종합병원급 건강검진… 실시간 공개 서비스 제공</td>\n",
       "      <td>economics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body            datetime  \\\n",
       "0  우리나라의 에너지 자원 해외 수입 의존도는 96%에 달한다. 특히 액화천연가스(LN... 2017-11-01 17:30:00   \n",
       "1  '공정한 병역(兵役) 실현'을 최우선 과제로 삼은 병무청이 올해 초부터 병역 판정 ... 2017-11-01 17:18:00   \n",
       "\n",
       "                                                link   press  \\\n",
       "0  http://news.chosun.com/site/data/html_dir/2017...  chosun   \n",
       "1  http://news.chosun.com/site/data/html_dir/2017...  chosun   \n",
       "\n",
       "                                 title      topic  \n",
       "0  전 세계 13개국서 탐사·개발… 모잠비크선 \"금세기 최대 규모\"  economics  \n",
       "1  총 25개 항목의 종합병원급 건강검진… 실시간 공개 서비스 제공  economics  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = dump_name + '/newsdata.json'\n",
    "data = pd.read_json(fname)\n",
    "data['datetime'] = pd.to_datetime(data['datetime'])\n",
    "data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'우리나라의 에너지 자원 해외 수입 의존도는 96%에 달한다. 특히 액화천연가스(LNG)는 전량 수입한다. 에너지 안보를 위해 안정적인 자원 확보는 무엇보다 중요하다. 해외 자원 개'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = dump_name + '/newsdata.txt'\n",
    "with open(fname, 'r') as f:\n",
    "    full_txt = f.read()\n",
    "full_txt[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `byte` Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['우리나라의 에너지 자원 해외 수입 의존도는 96%에 달한다. 특히 액화천연가스(LNG)는 전량 수입한다. 에너지 안보를 위해 안정적인 자원 확보는 무엇보다 중요하다. 해외 자원 개발 사업은 이제 선택이 아니라 필수가 됐다.올해로 창립 34주년을 맞은 한국가스공사는 과거 단순히 LNG를 도입·판매하던 것에서 벗어나 종합 에너지기업으로 성장하기 위해 에너지 자원 개발 역량을 키우는 데 집중하고 있다. 현재 전 세계 13개국에서 탐사, 개발·생산, LNG, 인프라 등 24개 사업을 진행하고 있다.가스공사가 해외에서 확보한 자원량은 지난해 기준 2억5158만t에 달한다. 한국이 8년 가까이 사용할 수 있는 규모다. 가스공사는 지난해 해외 사업에서 1조102억원의 매출을 기록했다. 올해 상반기엔 445억원의 영업이익을 기록했다.대표적인 해외 자원 개발 사업이 가스공사가 지난 2007년 지분 매입 계약을 한 모잠비크 4구역(Area 4) 가스전 개발 사업이다. 모잠비크 동쪽 해상에 있는 이 가스전은 \\'금세기 최대 규모의 가스전\\'으로 불린다. 매장량이 19억2000만t에 달한다. 가스공사는 이 가스전 지분 10%를 보유하고 있다. 가스공사의 보유 매장량을 국내 소비량으로 환산하면 5년 6개월을 쓸 수 있는 물량이다. 내년부터 본격적으로 개발이 시작되면 2022년부터 상업 생산이 이뤄질 예정이다.이라크 주바이르 유·가스전 개발 사업은 가스공사의 해외 자원 개발 사업 가운데 투자 회수율이 가장 높은 사업 가운데 하나다. 가스공사는 이 유전 지분의 약 23%를 확보하고 있다. 이곳에선 하루 최대 85만배럴의 원유가 생산된다. 가스공사는 2010년 자회사를 설립, 개발에 착수했다. 그동안 자회사에 3억7800만달러를 투자해 1단계로 올해 5월 약 2억달러를 회수했다.호주 북동부 퀸즈랜드주의 가스전 개발 및 액화플랜트 건설·운영사업도 벌이고 있다. 지분 15%를 확보, 2015년 9월 첫 LNG 생산을 개시했다.이 밖에도 가스공사는 우즈베키스탄 사상 최대 규모의 패키지형 에너지 프로젝트를 진행 중이다. 우즈베키스탄 수르길 가스전을 우즈베키스탄 석유공사와 공동 개발하고, 가스화학플랜트를 건설·운영해 화학제품을 생산·판매하고 남은 천연가스를 판매할 계획이다. 프로젝트 파이낸싱(PF) 방식으로 진행, 사업의 안정성을 확보했다. 롯데케미칼과 GS E&amp;R 등 국내 민간 기업과 동반 진출한 성공 사례로 평가받는다. 지난해 2월부터 상업 운전을 시작했다.신규 투자도 모색 중이다. 카타르·오만과의 LNG 도입 계약이 만료되는 2025년 이후 미국산 LNG 수입을 검토 중이다. 가스공사는 \"자원 개발 사업 구조조정 성과를 바탕으로 투자 여력을 확보해 핵심 사업 중심으로 해외 사업을 추진해 나갈 계획\"이라고 말했다.',\n",
       " '\\'공정한 병역(兵役) 실현\\'을 최우선 과제로 삼은 병무청이 올해 초부터 병역 판정 검사를 새롭게 개선하고, 병역 검사 대상자들에게 다양한 서비스를 제공하고 있다.병무청은 추가된 검사 항목으로 종합병원급 건강검진을 실시하고, 검진 결과서를 현장에서 발급, 상세하고 신속하게 검진 결과를 확인할 수 있도록 했다. 또 병역 판정 검사 대상자 가족들은 병무청 홈페이지나 모바일 앱을 통해 검사 과정을 실시간으로 지켜볼 수 있게 됐다.병무청은 지난해까지는 병역 판정 검사에서 20개 항목을 검사했다. 그러나 올해부터는 알코올성 간질환, 동맥경화, 지질대사질환, 심혈관계질환, 신장 기능 검사 등 5가지를 추가해 총 25개의 항목을 검사한다. 종합병원 건강검진엔 포함돼 있지 않은 후천성면역결핍증(HIV) 검사도 실시한다.이와 함께 결핵 예방과 국민 건강 증진을 위한 혈액검사(IGRA)도 올해 추가된 항목이다. 이를 통해 잠복 결핵이나 간 기능, 척추, 심장 등의 이상 여부를 판단할 수 있다. 과거엔 검사를 통해 신체 등위(等位) 판정이 주목적이었다면 올해부터는 신체 등위 판정에 건강검진까지 추가한 것이다.검사 과정은 \\'병역 판정 검사 과정 실시간 공개 서비스\\'를 통해 병역 의무자의 부모와 가족에게 실시간으로 공개된다. 병무청 홈페이지나 모바일 앱에 접속하면 건강검진 내용과 진행 과정을 바로 확인할 수 있다.검진 대상자 어머니 이모(46)씨는 \"아들이 병역 판정 검사를 받으러 가는 것을 보며 마음이 놓이지 않았는데, \\'병역 판정 검사 과정 실시간 공개 서비스\\'를 통해 스마트폰으로 검사 과정을 한눈에 확인할 수 있어 한결 마음이 놓였다\"고 말했다.병무청은 또 병역 판정 검사 결과를 활용해 병역 의무자에게 종합병원 건강검진 수준의 개인별 맞춤식 건강 정보 서비스를 제공한다. 질병의 원인과 증상, 치료, 예방 등 7개 항목 55종의 정보가 담겨 있다.건강검진 결과서 현장 발급 서비스도 올해 새로 시작했다. 병무청은 20쪽 분량의 건강검진 결과서를 현장에서 발급, 세부 검사 항목별 검사의 목적과 결과에 대한 임상적 의미와 개인별 상세 질병 건강 정보를 제공하고 있다.병무청은 또 병역 판정 검사 결과를 빅데이터로 활용해 병역 면탈을 사전에 차단하는 \\'디지털 포렌식 수사 기법\\'의 기반으로 삼기로 했다. 병역을 기피하거나 감면받을 목적으로 일부러 신체를 손상하는 행위, 고의로 전신 문신을 하거나 체중을 조절하는 행위, 정신질환·안과질환으로 위장하는 등 속임수를 쓰는 행위, 병역 판정 검사에서 다른 사람이 대신 검사를 받도록 하는 행위 등을 엄중히 단속·처벌할 방침이다.기찬수 병무청장은 \"종합 건강검진 차원의 병역 판정 검사와 맞춤형 건강 정보 제공은 병역의 의무를 이행하는 입영자와 가족 등에게 병역에 대한 불안을 해소하고 신뢰를 형성하는 기반이 될 수 있을 것\"이라며 \"국민의 신뢰를 얻고, 병무 행정의 투명성을 인정받아 \\'병역이 자랑스러운 대한민국\\'이 될 수 있도록 최선을 다하겠다\"고 말했다.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = dump_name + '/newsdata.dump'\n",
    "with open(fname, 'rb') as f:\n",
    "    full_list = pickle.load(f)\n",
    "    \n",
    "full_list[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow : Python 3.6 (conda env)",
   "language": "python",
   "name": "tf-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "1205px",
    "left": "0px",
    "right": "1026px",
    "top": "107px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": "20",
    "lenType": 16,
    "lenVar": "41"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
